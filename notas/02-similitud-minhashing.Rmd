# Similitud y minhashing

En la primera parte del curso tratamos un problema fundamental en varias tareas de análisis de datos: 

- ¿Cómo medir similitud entre objetos o casos?
- ¿Cómo encontrar vecinos cercanos en un conjunto de datos?
- ¿Cómo hacer uniones de tablas por similitud?

Algunos ejemplos son:

- Encontrar documentos similares en una colección de documentos. Esto puede 
servir para detectar
plagio, deduplicar noticias o páginas web, hacer *matching* de datos
de dos fuentes (por ejemplo, nombres completos de personas),
etc. Ver por ejemplo [Google News]((https://dl.acm.org/citation.cfm?id=1242610)).
- Encontrar usuarios similares (Netflix), en el sentido de que tienen gustos similares, o películas similares, en el sentido de qe le gustan a las mismas personas.
- Encontrar imágenes similares en una colección grande, ver por ejemplo [Pinterest](https://medium.com/@Pinterest_Engineering/detecting-image-similarity-using-spark-lsh-and-tensorflow-618636afc939).
- Uber: rutas similares que indican (fraude o abusos)[https://eng.uber.com/lsh/].
- Deduplicar registros de usuarios de algún servicio (por ejemplo, beneficiarios
de programas sociales).

Estos problemas no son triviales por dos razones:

- Los elementos que queremos comparar muchas veces están naturalmente representados en espacios de dimensión alta, y es relativamente costoso comparar un par (documentos, imágenes, usuarios, rutas). Muchas veces es preferible construir una representación más compacta y hacer comparaciones con las versiones comprimidas.
- Si la colección de elementos es grande ($N$), entonces el número de pares 
posibles es del orden de $N^2$, y no es posible hacer todas las posibles comparaciones para encontrar los elementos similares (por ejemplo, comparar
$100$ mil documentos, con unas $10$ mil comparaciones por segundo, tardaría alrededor de $5$ días).

Si tenemos que calcular *todas* las similitudes, no hay mucho qué hacer. Pero
muchas veces nos interesa encontrar pares de similitud alta, o completar tareas
más específicas como contar duplicados, etc. En estos casos, veremos que es
posible construir soluciones probabilísticas aproximadas para resolver estos
problemas de forma escalable. 

Aunque veremos más adelante métricas de similitud comunes como
la dada por la distancia euclideana o distancia coseno, por ejemplo, en 
esta primera parte nos concentramos en discutir similitud entre
pares de textos. Los textos los podemos ver como colecciones de palabras, o
de manera más general, como colecciones de cadenas.


## Similitud de conjuntos

Muchos de estos problemas de similitud se pueden pensar como 
problemas de similitud entre conjuntos. Por ejemplo, los documentos son conjuntos de palabras, conjuntos
de pares de palabras, sucesiones de caracteres,
una película se puede ver como el conjunto de personas a las que le gustó, o una ruta
como un conjunto de tramos, etc.

Hay muchas medidas que son útiles para cuantificar la similitud entre conjuntos. Una que es popular, y que explotaremos por sus propiedades, es la similitud de Jaccard:


```{block2, type='resumen'}
La **similitud de Jaccard** de los conjuntos $A$ y $B$ está dada por

$$sim(A,B) = \frac{|A\cap B|}{|A\cup B|}$$

```

Esta medida cuantifica qué tan cerca está la unión de $A$ y $B$ de su intersección. Cuanto más parecidos sean $A\cup B$ y $A\cap B$, más similares son los conjuntos. En términos geométricos, es el área de la intersección entre el área de la unión. 

#### Ejercicio {-}

Calcula la similitud de Jaccard entre los conjuntos $A=\{5,2,34,1,20,3,4\}$
 y $B=\{19,1,2,5\}$
 

```{r, collapse = TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
options(digits = 3)

sim_jaccard <- function(a, b){
    length(intersect(a, b)) / length(union(a, b))
}

sim_jaccard(c(0,1,2,5,8), c(1,2,5,8,9))
sim_jaccard(c(2,3,5,8,10), c(1,8,9,10))
sim_jaccard(c(3,2,5), c(8,9,1,10))
```


## Representación de documentos como conjuntos

Hay varias maneras de representar documentos como conjuntos. Las más simples son:

1. Los documentos son colecciones de palabras, o conjuntos de sucesiones de palabras de tamaño $n$.
2. Los documentos son colecciones de caracteres, o conjuntos de sucesiones de caracteres (cadenas) de tamaño $k$.


La primera representación se llama *representación de n-gramas*, y la segunda *representación de k-tejas*.

Nótese que en ambos casos podemos incluir información acerca del *orden* en el que
ocurren las palabras o caracteres, y no solamente su ocurrencia en el documento. Esto es útil para varios problemas.

Consideremos una colección de textos cortos:

```{r}
textos <- character(4)
textos <- c("el perro persigue al gato, pero no lo alcanza", 
            "el gato persigue al perro, pero no lo alcanza", 
            "este es el documento de ejemplo", 
            "el documento habla de perros, gatos, y otros animales")
```

Abajo mostramos la representacion en bolsa de palabras (1-gramas) y la representación en bigramas (2-gramas) de los primeros dos documentos:

```{r}
print("Bolsa de palabras:")
tokenizers::tokenize_ngrams(textos[1:2], n = 1) %>% map(unique)
print("2-gramas:")
tokenizers::tokenize_ngrams(textos[1:2], n = 2) %>% map(unique)
```

La representación en _k-tejas_ es otra posibilidad:

```{r}
calcular_tejas <- function(x, k = 2){
  tokenizers::tokenize_character_shingles(x, n = k, lowercase = FALSE,
    simplify = TRUE, strip_non_alpha = FALSE)
}
print("2-tejas:")
calcular_tejas(textos[1:2], k = 2) %>% map(unique)
print("4-tejas:")
calcular_tejas(textos[1:2], k = 4) %>% map(unique)
```


**Observaciones**:

1. Los _tokens_ son las unidades básicas de análisis. Los _tokens_ son palabras para los n-gramas (cuya definición no es del todo simple) y caracteres para las k-tejas. Podrían ser también oraciones, por ejemplo.
2. Nótese que en ambos casos es posible hacer algo de preprocesamiento para
obtener la representación. Transformaciones usuales son:

  - Eliminar puntuación y/o espacios. 
  - Convertir los textos a minúsculas.
  - Esto incluye decisiones acerca de qué hacer con palabras compuestas (por ejemplo, con un guión), palabras que denotan un concepto (Reino Unido, por ejemplo) y otros detalles.

3. Si lo que nos interesa principalmente
similitud textual (no significado, o polaridad, etc.) entre documentos, entonces podemos usar $k$-tejas, con un mínimo de preprocesamiento. Esta
representación es **simple y flexible** en el sentido de que se puede adaptar para documentos muy cortos (mensajes o tweets, por ejemplo), pero también para documentos más grandes.

Por estas razones, no concentramos por el momento en $k$-tejas


```{block2, type = 'resumen'}
**Tejas (shingles)**
  
Sea $k>0$ un entero. Las $k$-tejas ($k$-shingles) de un documento d
 es el conjunto de todas las corridas (distintas) de $k$
caracteres sucesivos.

Escogemos $k$ suficientemente grande, de forma que la probabilidad de que
una teja particular ocurra en un texto dado sea relativamente baja.
```


#### Ejemplo {-}
Documentos textualmente similares tienen tejas similares:

```{r, collapse = TRUE}
# calcular tejas
tejas_doc <- calcular_tejas(textos, k = 4)
# calcular similitud de jaccard entre algunos pares
sim_jaccard(tejas_doc[[1]], tejas_doc[[2]])
sim_jaccard(tejas_doc[[1]], tejas_doc[[3]])
sim_jaccard(tejas_doc[[4]], tejas_doc[[3]])
```

Podemos calcular todas las similitudes:

```{r}
tejas_tbl <- crossing(id_1 = 1:length(textos), id_2 = 1:length(textos)) %>%
  filter(id_1 < id_2) %>% 
  mutate(tejas_1 = tejas_doc[id_1], tejas_2 = tejas_doc[id_2])
tejas_tbl
```

```{r}
tejas_tbl %>% 
  mutate(sim = map2_dbl(tejas_1, tejas_2, ~sim_jaccard(.x, .y))) %>% 
  select(id_1, id_2, sim)
```

pero nótese que, como señalamos arriba, esta operación será muy
costosa incluso si la colección de textos es de tamaño moderado.


- Si los textos
son cortos, entonces basta tomar valores como $k=4,5$, pues hay un total de $27^4$ tejas
de tamaño $4$, y el número de tejas de un documento corto (mensajes, tweets) es mucho más bajo que
$27^4$ (nota: ¿puedes explicar por qué este argumento no es exactamente correcto?)

- Para documentos grandes, como noticias o artículos, es mejor escoger un tamaño más grande,
como $k=9,10$, pues en documentos largos puede haber cientos de miles
de caracteres. Si $k$ fuera más chica entonces una gran parte de las tejas aparecerá en muchos de los documentos, y todos los documentos tendrían similitud alta.

- Evitamos escoger $k$ demasiado grande, pues entonces los únicos documentos similares tendrían
que tener subcadenas largas exactamente iguales. Por ejemplo: "Batman y Robin" y "Robin y Batman" son muy
similares si usamos tejas de tamaño 4, pero son muy distintas si usamos tejas de tamaño 8:

#### Ejemplo {-}

```{r}
tejas_1 <- calcular_tejas("Batman y Robin", k = 4)
tejas_2 <- calcular_tejas("Robin y Batman", k = 4)
sim_jaccard(tejas_1, tejas_2)
tejas_1 <- calcular_tejas("Batman y Robin", k = 8)
tejas_2 <- calcular_tejas("Robin y Batman", k = 8)
sim_jaccard(tejas_1, tejas_2)
```



## Representación matricial

Podemos usar una matriz binaria para guardar todas las
representaciones en k-tejas de nuestra colección de documentos.

```{r}
dtejas_tbl <- tibble(id = paste0("doc_", 1:length(textos)), 
    tejas = tejas_doc) %>% 
  unnest_legacy %>% 
  unique %>% mutate(val = 1) %>% 
  pivot_wider(names_from = id, values_from = val, values_fill = list(val = 0)) %>% 
  arrange(tejas) # opcionalmente ordenamos tejas
dtejas_tbl
```


¿Cómo calculamos la similitud de Jaccard usando estos datos?

Calcular la unión e intersección se puede hacer haciendo OR y AND de las columnas, y
entonces podemos calcular la similitud
```{r}
inter_12 <- sum(dtejas_tbl$doc_1 & dtejas_tbl$doc_2)
union_12 <- sum(dtejas_tbl$doc_1 | dtejas_tbl$doc_2)
similitud <- inter_12/union_12
similitud # comparar con el número que obtuvimos arriba.
```

El cálculo para todos los documentos podríamos hacerlo con:

```{r}
mat_td <- t(dtejas_tbl %>% select(-tejas) %>% as.matrix)
1 - dist(mat_td, method = "binary")
```




## Reducción probabilística de dimensionalidad

Para una colección grande de documentos
la representación binaria de la colección de documentos 
puede tener un número muy grande de renglones. Puede ser posible
crear un número más chico de nuevos _features_ (ojo: aquí los renglones
son las "variables", y los casos son las columnas) con los que
sea posible obtener una buena aproximación de la similitud.

Los mapeos que usaremos son escogidos al azar, y son sobre
el espacio de enteros.

- Sea $\pi$ una permutación al azar de los renglones de la matriz.
- Permutamos los renglones de la matriz tejas-documentos según $\pi$.
- Definimos una nuevo descriptor, el **minhash** del documento: para cada documento (columna) $d$ de la matriz permutada, tomamos el entero $f_\pi (d)$ que da el 
número del primer renglón que es distinto de $0$.


#### Ejercicio {#ej1}

Considera la matriz de tejas-documentos para cuatro documentos y cinco tejas
dada a continuación, con las permutaciones $(2,3,4,5,1)$ (indica que el renglón
$2$ va al $1$, el $3$ a $2$, el $1$ al $5$, etc.) y $(2,5,3,1,4)$. Calcula el descriptor definido arriba.

```{r, echo = FALSE}
mat <- matrix(c(c(1,0,0,1), c(0,0,1,0), 
            c(0,1,0,1), c(1,0,1,1),
            c(0,0,1,0)), nrow = 5, ncol = 4, byrow = TRUE)
colnames(mat) <- c('d_1','d_2','d_3','d_4')
rownames(mat) <- c('abc', 'ab ','xyz','abx','abd')
mat
```


#### Ejemplo {-}

Ahora regresamos a nuestro ejemplo de $4$ textos chicos.
Por ejemplo, para una permutación tomada al azar:

```{r}
set.seed(321)
df_1 <- dtejas_tbl %>% sample_n(nrow(dtejas_tbl))
df_1
```

Los minhashes para cada documentos con estas permutaciones son:

```{r}
df_1 %>% summarise_at(vars(matches('doc')), detect_index, ~.x == 1) 
```

Ahora repetimos con otras permutaciones:

```{r}
calc_firmas_perm <- function(df, permutaciones){
    map(permutaciones, function(perm){
        # permutar matriz (quisiéramos evitar)
        df_1 <- df[order(perm), ]
        # encontrar el primer uno
        firma <- df_1 %>% summarise_at(vars(matches('doc')), detect_index, ~.x == 1)
        firma
    }) %>% bind_rows %>% 
    add_column(firma = paste0('h_', 1:length(permutaciones)), .before = 1)
}
set.seed(312)
num_perms <- 20
permutaciones <- map(1:num_perms, ~ sample.int(n = nrow(dtejas_tbl)))
firmas_perms <- calc_firmas_perm(dtejas_tbl, permutaciones)
firmas_perms
```



---

A esta nueva matriz le llamamos **matriz de firmas** de los documentos.  La firma de un documento es una sucesión de enteros.

Cada documento se describe ahora con `r nrow(firmas_perms)` entradas,
en lugar de `r nrow(df_1)`.

Nótese que por construcción, cuando dos documentos son muy similares,
es natural que sus columnas de firmas sean similares, pues la mayor parte
de los renglones de estos dos documentos son $(0,0)$ y $(1,1)$.
Resulta ser que podemos cuantificar esta probabilidad. Tenemos el siguiente
resultado simple pero sorprendente:

```{block2, type = 'resumen'}
Sea $\pi$ una permutación escogida al azar, y $a$ y $b$ dos columnas
dadas. Entonces
$$P(f_\pi(a) = f_\pi(b)) = sim(a, b)$$
donde $sim$ es la similitud de Jaccard basada en las tejas usadas.
Sean $\pi_1, \pi_2, \ldots \pi_n$ permutaciones escogidas al azar de
manera independiente. Si $n$ es grande, entonces por la ley de los grandes números
$$sim(a,b) \approx \frac{|\pi_j : f_{\pi_j}(a) = f_{\pi_j}(b)|}{n},$$
es decir, la similitud de Jaccard es aproximadamente la proporción 
de elementos de las firmas que coinciden.
```



### Ejemplo {-}

Antes de hacer la demostración, veamos como aplicaríamos a la matriz
de firmas que calculamos arriba. Tendríamos, por ejemplo :

```{r, collapse = TRUE}
mean(firmas_perms$doc_1 == firmas_perms$doc_2)
mean(firmas_perms$doc_1 == firmas_perms$doc_3)
mean(firmas_perms$doc_3 == firmas_perms$doc_4)
```

que comparamos con las similitudes de Jaccard

```{r}
sim_jaccard(tejas_doc[[1]], tejas_doc[[2]])
sim_jaccard(tejas_doc[[1]], tejas_doc[[3]])
sim_jaccard(tejas_doc[[4]], tejas_doc[[3]])
```

Ahora veamos qué sucede repetimos varias veces:

```{r, collapse = TRUE}
num_perms <- 20
firmas_rep <- map(1:50, function(i){
    perms <- map(1:num_perms, sample, x = 1:nrow(dtejas_tbl), size = nrow(dtejas_tbl))
    df_out <- calc_firmas_perm(dtejas_tbl, perms)    
    df_out$rep <- i
    df_out
})
  
map_dbl(firmas_rep, ~ mean(.x$doc_1 == .x$doc_2))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(2)
map_dbl(firmas_rep, ~ mean(.x$doc_3 == .x$doc_4))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(2)
```

Que indica que nuestro procedimiento da estimaciones razonables
de las similitudes de Jaccard.

*Observación*: si la similitud de dos documentos es cero, entonces
este procedimiento siempre da la respuesta exacta. ¿Por qué?

---

Ahora damos un argumento para demostrar este resultado.
Consideremos dos columnas $a,b$ de la matriz
de 0's y 1's, con conjuntos de tejas asociados $A,B$.

- Permutamos los reglones de las dos columnas $a$ y $b$.
- Sea $k$ la posición donde aparece el primer $(0,1)$, $(1,0)$ o $(1,1)$.
- Hay tantos renglones $(1,1)$ como elementos en $A\cap B$. Y hay tantos
renglones  $(0,1)$, $(1,0)$ o $(1,1)$ como elementos en $A\cup B$.
- Todos estos $|A\cup B|$ reglones tienen la misma probabilidad de aparecer
en la posición $k$.
- Entonces, la probabilidad condicional de que el renglón $k$ sea de tipo $(1,1)$, dado que es de algún tipo de $(1,0), (0,1), (1,1)$, es 
$$\frac{|A\cap B|}{|A\cup B|},$$
que es la similitud de Jaccard de los dos documentos.

## Cálculo de firmas por documento

Podemos evitar permutar matrices (una operación costosa) de varias formas.
Una manera de hacer esto es trabajar documento por documento, lo cual es natural para colecciones 
grandes de textos.

En este caso, es fácil ver que para una permutación dada, la firma de un documento puede encontrarse
de la siguiente forma:

1. Para la columna $d$ (documento), tomamos todos los valores de los índices igual a 1. 
2. Aplicamos la permutación $\pi$ a cada uno de estos índices.
3. Calculamos el mínimo de los valores resultantes para obtener $f_\pi (d)$.
4. Repetimos para cada permutación para obtener la firma del documento.


Escribimos nuestra función para calcular tejas y calcular firmas:

```{r}
crear_tejas_num <- function(textos, k = 4){
    # las tejas serán convertidas a enteros
    num_docs <- length(textos)
    # crear tejas (codificando a números)
    tejas <- calcular_tejas(textos, k = k) %>% map(unique)
    tejas_df <- tibble(texto_id = 1:num_docs, tejas = tejas) %>%
        unnest_legacy %>% 
        mutate(tejas_num = as.numeric(factor(tejas))) %>% 
        group_by(texto_id) %>% #agrupamos las tejas de cada doc
        summarise(tejas = list(tejas_num))
    tejas_df
}
calcular_firmas_doc <- function(tejas_df, hash_funs){
    # Calcula firmas
    num_docs <- nrow(tejas_df)
    num_hashes <- length(hash_funs)
    tejas <- tejas_df$tejas
    firmas <- vector("list", num_docs)
    for(i in 1:num_docs){
        firmas[[i]] = map_dbl(hash_funs, ~ min(.x(tejas[[i]])))
    }
    tibble(doc_id = 1:num_docs, firma = firmas)
}
```

Nótese que en el código de arriba utiiizamos funciones en lugar de vectores para
representar permutaciones. Este correspondencia es como sigue:

```{r}
# uno se queda fijo, 4 renglon va a posición 2, 5 a posición 3, etc:
perm = c(1,4,5,2,3)
fun_perm <- function(z) perm[z]
fun_perm(3); fun_perm(4)
```

```{r}
# convertimos vectores de permutaciones en funciones
perm_funs <- map(permutaciones, ~ function(z) .x[z])
# creamos tejas y calculamos firmas
crear_tejas_num(textos, k = 4) %>% 
  calcular_firmas_doc(perm_funs) %>%                 
  unnest_legacy %>% 
  # esta parte es para poner en forma ancha:
  group_by(doc_id) %>% 
  mutate(hash = row_number()) %>% 
  pivot_wider(names_from=doc_id, values_from = firma)
```

**Observación**: nótese que no estamos permutando ningún vector o matriz en este cálculo. Esto
nos da la siguiente idea importante:



## Funciones hash

Con la técnica de firmas derivadas de permutaciones logramos
reducir la dimensionalidad de nuestro problema, y este es un primer paso
para encontrar pares similares en una colección grande de documentos. Sin embargo,
es necesario simular
y almacenar las distintas permutaciones (quizá usamos cientos, para estimar
con más precisión las similitudes) que vamos a utilizar. Estas permutaciones
son relativamente grandes y quizá podemos encontrar una manera más rápida de "simular"
las permutaciones.


### Ejercicio{-}
En nuestro ejemplo anterior, tenemos $107$ tejas. Consideramos una funciones de
la forma (como se sugiere en [@mmd]):

$$h(x) = (ax + b) \bmod 107$$
donde escogemos $a$ al azar entre 1 y 106, y $b$ se escoge al azar
entre $0$ y $106$. Recuerda que $A \bmod p$ es el residuo que se obtiene al dividir $A$
entre $p$.¿Por qué es apropiada una función de este tipo?

- Demuestra primero que
la función $h(x)$ es una permutación de los enteros $\{ 0,1,\ldots, 106 \}$. Usa el
hecho de que $107$ es un número primo. 
- Si escogemos $a, b$ al azar, podemos generar distintas permutaciones.
- Esta familia de funciones no dan todas las posibles permutaciones, pero
pueden ser suficientes para nuestros propósitos, como veremos más adelante.

**Observación**: si $p$ no es primo, nuestra familia tiene el defecto de que
algunas funciones pueden nos ser permutaciones. Por ejemplo,
si 
$$h(x) = 4x + 1 \bmod 12,$$
entonces $h$ mapea el rango $\{0,1,\ldots, 11\}$ a
```{r}
h <- function(x){  (4*x +1) %% 12}
h(0:11)
```

---

### Ejemplo {-}

Vamos a resolver nuestro problema simple usando funciones hash modulares:

```{r}
hash_simple <- function(p){
  # seleccionamos al azar dos enteros menores que p
  # p es un número primo
  a <- sample.int(p - 1, 2)
  hash_fun <- function(x) {
        # restamos y sumamos uno para mapear a enteros positivos
        ((a[1]*(x-1) + a[2]) %% p) + 1
    }
  hash_fun
}
set.seed(132)
hash_f <- map(1:2, ~ hash_simple(p = 107))
```

Podemos examinar algunas de estas funciones:
```{r}
hash_f[[1]](1:107)
hash_f[[1]](1:107) %>% duplicated %>% any
hash_f[[2]](1:107) %>% duplicated %>% any
```

No es neceario reescribir nada en nuestro código. Pero en lugar de usar permutaciones,
usaremos familias de funciones hash extraidas al azar como mostramos arriba
Reescribimos entonces nuestra función *calc_firmas* para usar las funciones
hash en lugar de permutaciones, lo cual evita simular y guardar las permutaciones.


```{r}
set.seed(2851)
hash_funs <- map(1:20, ~ hash_simple(p = 107))
# mismo coigo creamos tejas y calculamos firmas
firmas_doc <- crear_tejas_num(textos, k = 4) %>% 
  calcular_firmas_doc(hash_funs) %>%                 
  unnest_legacy %>% 
  group_by(doc_id) %>% 
  mutate(hash = row_number()) %>% 
  pivot_wider(names_from=doc_id, values_from = firma, names_prefix = "doc_")
firmas_doc
```

Y checamos 

```{r, collapse = TRUE}
mean(firmas_doc$doc_1 == firmas_doc$doc_2)
mean(firmas_doc$doc_1 == firmas_doc$doc_3)
mean(firmas_doc$doc_3 == firmas_doc$doc_4)
```


Cuando el número de tejas no exactamente un primo, hacemos lo siguiente:

```{block2, type = 'resumen'}
**Hash de tejas numeradas**
    
 Supongamos que tenemos $\{0,1,\ldots, m-1\}$ tejas numeradas. Seleccionamos un
primo $p\geq m$, y definimos las funciones
$$H =\{ h_{a,b}(x) = (ax + b) \bmod p \}$$
En lugar de escoger una permutación al azar, escogemos
$a \in \{1,\cdots, p-1\}$, y $b \in \{0,\cdots, p-1\}$ al azar. Usamos
en el algoritmo estas funciones $h_i$ como si fueran permutaciones.
```

**Observación**: El único detalle que hay que observar aquí es que los valores hash o cubetas
a donde se mapean las tejas ya no están en el rango $\{0,1,\ldots, m-1\}$ , de manera
que no podemos interpretar como permutaciones. Pero pensando un poco vemos 
no hay ninguna razón para interpretar los valores
de $h_i(r)$ como renglones de una matriz permutadas, y no necesariamente
$h_i$ tiene que ser una permutación de los renglones. $h_i$ puede ser una
función que mapea renglones (tejas) a un rango grande de enteros, de forma
que la probabilidad de que distintos renglones sean mapeados a un mismo hash 
sea muy baja.

Obtener el minhash es simplemente encontrar el mínimo entero de los valores hash
que corresponden a tejas que aparecen en cada columna.






### Funciones hash: discusión {-}

En consecuencia, como solución general para nuestro problema de minhashing podemos seleccionar un primo $p$ muy grande y utiliar la familia de congruencias $ax+b\bmod p$, seleccionando
$a$ y $b$ al azar (ver [implementación de Spark](https://github.com/apache/spark/blob/v2.1.0/mllib/src/main/scala/org/apache/spark/ml/feature/MinHashLSH.scala), donde se utiliza un primo fijo MinHashLSH.HASH_PRIME) .

Otro enfoque es hacer hash directamente de las tejas (caracteres).
En este caso, buscamos una función
hash de cadenas a enteros grandes que "revuelva" las cadenas a un rango
grande de enteros. 
Es importante la calidad de la función hash, pues no queremos tener demasiadas
colisiones aún cuando existan patrones en nuestras tejas.

Por ejemplo,
podemos utilizar la función *hash_string* del paquete textreuse [@R-textreuse] (implementada
en C++):

```{r, collapse = TRUE}
textreuse::hash_string('a')
textreuse::hash_string('b')
textreuse::hash_string('El perro persigue al gato') 
textreuse::hash_string('El perro persigue al gat') 
``` 

Para obtener otras funciones hash, podemos usar una técnica distinta. Escogemos
al azar un entero, y hacemos bitwise xor con este entero al azar. 
En laa implementación de *textreuse*, por ejemplo, se hace:

```{r}
set.seed(123)
generar_hash_tr <- function(){
    r <- as.integer(stats::runif(1, -2147483648, 2147483647))
    funcion_hash <- function(x){
        bitwXor(textreuse::hash_string(x), r)    
    }
    funcion_hash
}
h_1 <- generar_hash_tr()
h_2 <- generar_hash_tr()
h_1("abcdef")
h_2("abcdef")
system.time(h_1(rep(textos, 100000)))
```

Otra opción es utilizar como función básica otra función hash estándar, como *murmur32*
o *xxhash32* (la segunda es más rápida que la primera), que también pueden
recibir semillas para obtener distintas funciones:

```{r}
set.seed(123)
generar_hash <- function(){
    r <- as.integer(stats::runif(1, 1, 2147483647))
    funcion_hash <- function(shingles){
        digest::digest2int(shingles, seed =r) 
    }
    funcion_hash
}
h_1 <- generar_hash()
h_2 <- generar_hash()
h_1("abcdef")
h_2("abcdef")
system.time(h_1(rep(textos, 100000)))
```

### Ejemplo {-}
Usando estas funciones hash de cadenas, nuestro ejemplo ser vería como sigue

```{r}
crear_tejas_str <- function(textos, k = 4){
    # las tejas serán convertidas a enteros
    num_docs <- length(textos)
    # crear tejas (codificando a números)
    tejas <- calcular_tejas(textos, k = k)
    tejas_df <- tibble(texto_id = 1:num_docs, tejas = tejas)
    tejas_df
}
```


Y ahora calculamos las firmas minhash:

```{r}
set.seed(2851)
hash_funs <- map(1:20, ~ generar_hash())
# mismo coigo creamos tejas y calculamos firmas
firmas_doc <- crear_tejas_str(textos, k = 4) %>% 
  calcular_firmas_doc(hash_funs) %>%                 
  unnest_legacy %>% 
  group_by(doc_id) %>% 
  mutate(hash = row_number()) %>% 
  pivot_wider(names_from=doc_id, values_from = firma, names_prefix = "doc_")
firmas_doc
```

Podemos estimar similitudes igual que en los ejemplos anteriores:

```{r, collapse = TRUE}
mean(firmas_doc$doc_1 == firmas_doc$doc_2)
mean(firmas_doc$doc_1 == firmas_doc$doc_3)
mean(firmas_doc$doc_3 == firmas_doc$doc_4)
```







## Ejemplo: tweets 

Ahora buscaremos tweets similares en una colección de un [concurso de
kaggle](https://www.kaggle.com/rgupta09/world-cup-2018-tweets/home?utm_medium=email&utm_source=mailchimp&utm_campaign=datanotes-20180823).

```{r, message=FALSE}
ruta <- "../datos/FIFA.csv"
gc_ruta <- "https://storage.cloud.google.com/metodos-analiticos/FIFA.csv"
if(!file.exists(ruta)){
    download.file(gc_ruta, ruta)
} else {
    fifa <- read_csv(ruta)
}
tw <- fifa$Tweet
tw[1:10]
```

```{r firmas_strdoc}
set.seed(91922)
hash_f <- map(1:50, ~ generar_hash())
system.time(tejas_tbl <- crear_tejas_str(tw[1:200000], k = 5))
system.time(firmas <- calcular_firmas_doc(tejas_tbl, hash_f))
```

Por ejemplo, ¿cuáles son tweets similares al primero?

```{r}
similitudes <- map_dbl(1:length(firmas$firma), ~ mean(firmas$firma[[.x]] == firmas$firma[[1]]))
indices <- which(similitudes > 0.4)
length(indices)
similares <- tibble(tweet = tw[indices],
           jacc_estimada = similitudes[indices]) %>%
    arrange(jacc_estimada)
DT::datatable(similares)
```


## Ejemplo: tweets, usando textreuse

En el paquete *textreuse*, se usa un algoritmo por documento, y hace
hash directamente de las cadenas de las tejas

```{r}
library(textreuse)
minhash <- minhash_generator(50)
```

```{r textreuse}
# este caso ponemos en hash_func los minhashes, para después
# usar la función pairwise_compare (que usa los hashes)
options("mc.cores" = 4L)
system.time(
corpus_tweets <- TextReuseCorpus(text = tw[1:200000], 
    tokenizer = calcular_tejas, k = 5,
    hash_func = minhash, keep_tokens = TRUE,
    keep_text = TRUE, skip_short = FALSE)
)
```

Busquemos tweets similares a uno en particular

```{r}
corpus_tweets[[1]]$content
min_hashes <- hashes(corpus_tweets)
similitud <- map_dbl(min_hashes, ~ mean(min_hashes[[1]] == .x))
indices <- which(similitud > 0.4)
length(names(indices))
```

```{r}
names(indices)[1:5]
similitud[indices][1:5]
map(names(indices), ~ corpus_tweets[[.x]]$content)[1:5]
```


¿Cuáles son las verdaderas distancias de Jaccard? Por ejemplo,

```{r}
jaccard_similarity(
  calcular_tejas(corpus_tweets[["doc-1"]]$content, k = 5),
  calcular_tejas(corpus_tweets[["doc-417"]]$content, k = 5)
  )
```

```{r}
jaccard_similarity(
  calcular_tejas(corpus_tweets[["doc-1"]]$content, k = 5),
  calcular_tejas(corpus_tweets[["doc-5"]]$content,  k = 5)
  )
```

**Observación**: Una vez que calculamos los que tienen similitud
aproximada $>$ $0.4$, podemos calcular la función de Jaccard exacta
para los elementos similares resultantes.


## Buscando vecinos cercanos

Aunque hemos reducido el trabajo para hacer comparaciones de documentos,
no hemos hecho mucho avance en encontrar todos los pares similares
de la colección completa de documentos. Intentar calcular similitud
para todos los pares (del orden $n^2$) es demasiado trabajo:

```{r simstextreuse}
system.time(
pares  <- pairwise_compare(corpus_tweets[1:200], ratio_of_matches) %>%
      pairwise_candidates()
)
pares <- pares %>% filter(score > 0.20) %>% arrange(desc(score)) 
```

Nótese que si tuviéramos $10$ veces más tweets (una fracción todavía del conjunto completo) el número de comparaciones se multiplica por $100$ aproximadamente.
En la siguiente parte veremos como aprovechar estos minhashes para hacer una
búsqueda eficiente de pares similares.

## Locality sensitive hashing (LSH) para documentos

Calcular todas las posibles similitudes de una
colección de un conjunto no tan grande de documentos es difícil. Sin
embargo, muchas veces lo que nos interesa es simplemente agrupar
colecciones de documentos que tienen alta similitud (por ejemplo para
deduplicar, hacer clusters de usuarios muy similares, etc.).

Una técnica para encontrar vecinos cercanos de este tipo es Locality Sensitive
Hashing (LSH), que generalizamos en la sección siguiente. Comenzamos
construyendo LSH basado en las firmas de minhash. La idea general
es: 

```{block2, type = 'resumen'}
**Idea general de Minshashing LSH**

- Recorremos la matriz de firmas documento por documento
- Asignamos el documento a una cubeta dependiendo de sus valores minhash (su firma).
- Todos los pares de documentos que caen en una misma cubeta son candidatos a pares similares. Generalmente tenemos mucho menos candidatos que el total de posibles pares.
- Checamos todos los pares candidatos dentro de cada cubeta que contenga más de un
elemento (calculando similitud de Jaccard original)
```

Nótese que con este método hemos resuelto de manera aproximada nuestro problema 
de encontrar pares similares. En la siguiente parte discutiremos cómo decidir
si es o no una buena aproximación. Veremos también formas de diseñar las cubetas para obtener candidatos con la
similitud que busquemos (por ejemplo, mayor a $0.5$, mayor a $0.9$, etc.). 

Antes veremos algunas posibilidades construidas a mano para lograr
nuestro objetivo. Usaremos en estos la implementación del algoritmo por documento.

### Ejemplo: todos los minshashes coinciden

Calculamos los minhashes, y creamos una cubeta para cada minhash diferente. Los
candidatos a similitud son pares que caen en una misma cubeta. Como usamos todos
los hashes, los candidatos tienden a ser textos muy similares.

Usamos un primo chico para leer los resultados más fácilmente:

```{r}
textos_dup <- c(textos,  textos[2], "este es el Documento de Ejemplo" ,
                paste0(textos[2], '!'), 'texto diferente a todos', "un gato negro",
                "mi gato negro")
textos_dup
set.seed(21)
hash_f <- map(1:20, ~ generar_hash())
tejas <- crear_tejas_str(textos_dup, k = 4)
firmas <- calcular_firmas_doc(tejas, hash_f)
firmas_2 <- firmas %>% 
    mutate(cubeta = map_chr(firma, paste, collapse ="-")) %>% 
    select(-firma)
firmas_2
```

Podemos hacer hash de la cadena de la cubeta:


```{r}
firmas_2 <- firmas %>% 
    mutate(cubeta = map_chr(firma, paste, collapse ="-")) %>% 
    mutate(cubeta = digest::digest2int(cubeta)) %>% 
    select(-firma)
firmas_2
```


Ahora agrupamos por cubetas:

```{r}
cubetas_df <- firmas_2 %>% group_by(cubeta) %>% 
    summarise(docs = list(doc_id)) %>% 
    mutate(n_docs = map_int(docs, length)) 
```

y filtramos las cubetas con más de un elemento:

```{r}
candidatos <- cubetas_df %>% filter(n_docs > 1)
candidatos
```

Y los candidatos de alta similud se extraen de estas cubetas
que tienen más de $1$ elemento. Son:

```{r}
candidatos$docs
```

Ahora podemos extraer los pares de similitud alta:

```{r}
extraer_pares <- function(candidatos, cubeta, docs, textos = NULL){
   enq_cubeta <- enquo(cubeta)
   enq_docs <- enquo(docs)
   pares <- candidatos %>% 
    group_by(!!enq_cubeta) %>% 
    mutate(pares = map(!!enq_docs, ~ combn(sort(.x), 2, simplify = FALSE))) %>%
    select(!!enq_cubeta, pares) %>% unnest_legacy %>% 
    mutate(a = map_int(pares, 1)) %>% 
    mutate(b = map_int(pares, 2)) %>% 
    select(-pares) %>% ungroup %>% select(-!!enq_cubeta) %>% 
    unique #quitar pares repetidos
   if(!is.null(textos)){
       pares <- pares %>% mutate(texto_a = textos[a], texto_b = textos[b])
   }
   pares %>% ungroup 
}
DT::datatable(extraer_pares(candidatos, cubeta, docs, textos = textos_dup))
```

### Ejemplo: algún grupo de minhashes coincide

Si quisiéramos capturar como candidatos pares de documentos con similitud
más baja, podríamos pedir que coincidan solo algunos de los hashes. Por ejemplo,
para agrupar textos con algún grupo de $2$ minshashes iguales, podríamos hacer:

```{r}
particion <- split(1:12, ceiling(1:12 / 2))
particion
separar_cubetas_fun <- function(particion){
    function(firma){
        map_chr(particion, function(x){
            prefijo <- paste0(x, collapse = '')
            cubeta <- paste(firma[x], collapse = "-")
            paste(c(prefijo, cubeta), collapse = '|')
        })
    }
}
sep_cubetas <- separar_cubetas_fun(particion)
sep_cubetas(firmas$firma[[1]])
firmas_3 <- firmas %>% 
    mutate(cubeta = map(firma, sep_cubetas)) %>% 
    select(-firma) %>% unnest_legacy
firmas_3
```

Ahora agrupamos por cubetas:

```{r}
cubetas_df <- firmas_3 %>% group_by(cubeta) %>% 
    summarise(docs = list(doc_id)) %>% 
    mutate(n_docs = map_int(docs, length)) 
```

y filtramos las cubetas con más de un elemento:

```{r}
candidatos <- cubetas_df %>% filter(n_docs > 1)
candidatos
```

```{r}
pares_candidatos <- extraer_pares(candidatos, cubeta, docs, textos = textos_dup) %>% 
                  arrange(texto_a)
DT::datatable(pares_candidatos)
```

Ahora podemos calcular similitud exacta y agrupar:

```{r}
pares_scores <- pares_candidatos %>% 
  mutate(score = map2_dbl(texto_a, texto_b,
  ~ sim_jaccard(calcular_tejas(.x, 5), calcular_tejas(.y, 5))))
DT::datatable(pares_scores)
```

Por ejemplo, podemos hacer clustering con las similitudes de Jaccard:

```{r}
library(igraph)
pares_dis <- pares_scores %>% select(a, b, score) 
pares_g <- graph_from_data_frame(pares_dis, directed = FALSE)
wc <- cluster_label_prop(pares_g, weights = pares_g$score)
wc
```
